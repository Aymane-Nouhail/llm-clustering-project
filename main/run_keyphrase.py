import numpy as np
import os
import pandas as pd
# Import modules from src
from src.config import (
    OPENAI_API_KEY, DATA_CACHE_PATH, DEFAULT_N_CLUSTERS,
    KP_PROMPT_TEMPLATE
)
from src.data import load_dataset
from src.llm_service import LLMService
# Import the updated function from src
from src.clustering_methods.keyphrase_expansion import cluster_via_keyphrase_expansion
from src.metrics import calculate_clustering_metrics

# Define the path for the metrics CSV file (use the same path)
METRICS_CSV_PATH = "clustering_metrics_results.csv"

def run_keyphrase_expansion_experiment(dataset_name):
    print("\n--- Running Keyphrase Expansion Experiment ---")

    # --- Configuration and Setup ---
    api_key = OPENAI_API_KEY
    if not api_key:
        print("OpenAI API Key not found.")
        return
    
    llm_service = LLMService(api_key)
    if not llm_service.is_available():
        print("LLM Service could not be initialized.")
        return
    
    embedding_model_instance = llm_service.get_embedding_model()
    if embedding_model_instance is None:
        print("Embedding model not available.")
        return

    # --- Load Data ---
    print("\nLoading data and embeddings...")
    features, labels, documents = load_dataset(
        dataset_name,
        cache_path=DATA_CACHE_PATH, 
        embedding_model=embedding_model_instance
    )
    
    if features.size == 0 or labels.size == 0 or not documents:
        print("Data loading failed.")
        return
    
    labels_np = np.array(labels)
    
    # Determine the number of clusters from the true labels
    n_clusters = len(np.unique(labels_np))
    print(f"Target number of clusters: {n_clusters}")

    # --- Run LLM Method 1: Keyphrase Expansion ---
    print(f"\nRunning Method 1: Keyphrase Expansion...")
    
    # Define the desired CSV output path for keyphrases generated by this method
    KEYPHRASE_OUTPUT_CSV = dataset_name + "_keyphrase_expansions_output.csv"
    
    # cluster_via_keyphrase_expansion now returns a dictionary of assignments
    keyphrase_assignments_dict = cluster_via_keyphrase_expansion(
        documents, features, n_clusters, llm_service, KP_PROMPT_TEMPLATE,
        keyphrase_output_csv_path=KEYPHRASE_OUTPUT_CSV # Pass the path here
    )
    
    # --- Evaluate and Report for each clustering approach ---
    all_metrics = []
    
    for method_name, assignments in keyphrase_assignments_dict.items():
        if assignments is not None:
            print(f"\nEvaluating {method_name} approach...")
            metrics = calculate_clustering_metrics(labels_np, assignments, n_clusters)
            
            # Create a metrics entry for this method
            metrics_data = {
                'Dataset': dataset_name,
                'Method': f'Keyphrase Expansion - {method_name}',
                'Status': "Success",
                **metrics
            }
            all_metrics.append(metrics_data)
        else:
            print(f"\nSkipping evaluation for {method_name} approach (no results).")
            # Add failed entry
            metrics_data = {
                'Dataset': dataset_name,
                'Method': f'Keyphrase Expansion - {method_name}',
                'Status': "Failed",
            }
            all_metrics.append(metrics_data)
    
    # --- Save All Metrics to CSV ---
    try:
        df_metrics = pd.DataFrame(all_metrics)
        
        # Append logic remains the same - check if file exists to write header
        if not os.path.exists(METRICS_CSV_PATH):
            df_metrics.to_csv(METRICS_CSV_PATH, index=False, mode='w', header=True)
            print(f"\nCreated {METRICS_CSV_PATH} and saved metrics.")
        else:
            df_metrics.to_csv(METRICS_CSV_PATH, index=False, mode='a', header=False)
            print(f"\nAppended metrics to {METRICS_CSV_PATH}.")
    except Exception as e:
        print(f"\nError saving metrics to CSV: {e}")

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        dataset_name = sys.argv[1]
        run_keyphrase_expansion_experiment(dataset_name)
    else:
        run_keyphrase_expansion_experiment("tweet")

