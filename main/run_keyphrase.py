import numpy as np
import os
import pandas as pd
# Import modules from src
from src.config import (
    OPENAI_API_KEY, DATA_CACHE_PATH, DEFAULT_N_CLUSTERS,
    KP_PROMPT_TEMPLATE
)
from src.data import load_dataset
from src.llm_service import LLMService
# Import the updated function from src
from src.clustering_methods.keyphrase_expansion import cluster_via_keyphrase_expansion
from src.metrics import calculate_clustering_metrics

# Define the path for the metrics CSV file (use the same path)
METRICS_CSV_PATH = "clustering_metrics_results.csv"

def run_keyphrase_expansion_experiment(dataset_name):
    print("\n--- Running Keyphrase Expansion Experiment ---")
    global METRICS_CSV_PATH
    METRICS_CSV_PATH = dataset_name + "_" + METRICS_CSV_PATH
    # --- Configuration and Setup ---
    api_key = OPENAI_API_KEY
    if not api_key:
        print("OpenAI API Key not found.")
        return

    llm_service = LLMService(api_key)
    if not llm_service.is_available():
        print("LLM Service could not be initialized.")
        return

    embedding_model_instance = llm_service.get_embedding_model()
    if embedding_model_instance is None:
         print("Embedding model not available.")
         return

    # --- Load Data ---
    print("\nLoading data and embeddings...")
    features, labels, documents = load_dataset(dataset_name,cache_path=DATA_CACHE_PATH, embedding_model=embedding_model_instance)

    if features.size == 0 or labels.size == 0 or not documents:
        print("Data loading failed.")
        return

    labels_np = np.array(labels)

    # Determine the number of clusters from the true labels
    n_clusters = len(np.unique(labels_np))
    print(f"Target number of clusters: {n_clusters}")


    # --- Run LLM Method 1: Keyphrase Expansion ---
    print(f"\nRunning Method 1: Keyphrase Expansion...")
    # Define the desired CSV output path for keyphrases generated by this method
    KEYPHRASE_OUTPUT_CSV = "keyphrase_expansions_output.csv"
    # cluster_via_keyphrase_expansion returns a full assignments array (with -1 for failed docs)
    keyphrase_assignments = cluster_via_keyphrase_expansion(
        documents, features, n_clusters, llm_service, KP_PROMPT_TEMPLATE,
        keyphrase_output_csv_path=KEYPHRASE_OUTPUT_CSV # Pass the path here
    )

    # --- Evaluate and Report ---
    assignments_np = np.array(keyphrase_assignments)
    metrics = calculate_clustering_metrics(labels_np, assignments_np, n_clusters)


    # --- Save Metrics to CSV ---
    metrics_data = {
        'Dataset': dataset_name,
        'Method': 'Keyphrase Expansion',
        'Status': "Success",
        **metrics
    }

    try:
        df_metrics = pd.DataFrame([metrics_data])

        # Append logic remains the same - check if file exists to write header
        if not os.path.exists(METRICS_CSV_PATH):
            df_metrics.to_csv(METRICS_CSV_PATH, index=False, mode='w', header=True)
            print(f"\nCreated {METRICS_CSV_PATH} and saved metrics.")
        else:
            df_metrics.to_csv(METRICS_CSV_PATH, index=False, mode='a', header=False)
            print(f"\nAppended metrics to {METRICS_CSV_PATH}.")

    except Exception as e:
        print(f"\nError saving metrics to CSV: {e}")


if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        dataset_name = sys.argv[1]
        run_keyphrase_expansion_experiment(dataset_name)
    else:
        run_keyphrase_expansion_experiment("tweet")

